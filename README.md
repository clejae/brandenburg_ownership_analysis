## Data preparation 
The data were provided in the Norm-based Data Interface (NAS) format by the Landesvermessung und Geobasisinformation Brandenburg (LGB). They came as 44 separate zipped XML-files. These files were imported into a PostgreSQL database using PostgreSQL Version 11 and the [norGIS-ALKIS tool ](http://norbit.de/68/). Until this point, we followed the workflow of [MÃ¼ller et al. (2021)](https://doi.org/10.22004/ag.econ.311013, who also provide a detailed description of these steps .

After creating the database, the parcel data with ownership information could be found under public --> v_eigentuemer. We exported the parcel data from the database using the algorithm "pgsql2shp" from BostonGIS, which can be downloaded from [BostonGIS](http://www.bostongis.com/pgsql2shp_shp2pgsql_quickguide_20.bqg). The following steps were taken:
	1. Open cmd
	2. Cd C:\Path\to\pgsql2shp.exe  
	3. Run:
	pgsql2shp -f "C:\Path\to\output_file\v_eigentuemer_bb_pgsql2shp.shp" -h localhost -u postgres -P password123 database_name public.v_eigentuemer 
The exported file was then used for further processing in Python.

## Description of original data
The shapefile had the following attributes:
OCG_FID = unique ID for parcels
EIGENTUEME = Owner information. For private people the names, birthdates and addresses were provided. For all others only names and addresses were provided. A single parcel could be owned by multiple different owners (i.e. shared ownership).
AMTLFLSFL = Area of parcel.
geomeotry = Geographic coordinates of parcel boundaries.
... and other columns that were not of interest to us.

## Data Preprocessing
The pre-processing and analysis of the data was done in several steps and mostly automatically. In some cases, however, manual input had to be made. Each major processing step was also divided into several sub-steps to allow repetition of the individual steps if necessary. We decided to save the results of all sub-steps as separate files. This made it possible to enter each processing step without going through the entire previous workflow again. However, this increased the storage space required.

### 1. Separating owner information from geometries and preliminary cleaning of owner strings
There were several problems with the owner information. The same owner could appear with multiple addresses throughout the dataset. Names, dates of birth and addresses could be spelled differently due to spelling errors and different abbreviations, and an owner could own both individual parcels and parcels with common ownership. Therefore, we had to clean up the owner information. To reduce loading times, we separated the geometries and the owner information into a shapefile that contained only the geographical information and a csv file that contained only the owner information. In the csv file we separated the names of the owners (+birth dates in the case of private individuals) from the addresses.

### 2. Stretching data frame and cleaning addresses
An owner could appear in the data frame as a single owner of a parcel and as one owner of several owners of a parcel with common ownership. Furthermore, this owner could appear in different spellings. These problems made it necessary to standardise the different spellings (which was done later). To this end, we extended the data set by separating all owners of parcels with common ownership. Each of them received a new data entry in the extended dataset. After extending the dataset, we cleaned up all addresses.

### 3. Preliminary owner name classification
We used the list of all stretched owners to classify them into a set of 29 preliminary classes. These classes helped in the further processing steps to perform various operations on the different classes. The preliminary classification was based on a classification of [business forms](https://www.praxis-agrar.de/betrieb/recht/rechtsformen-landwirtschaftlicher-unternehmen) - an overview of this can also be found [here](). We we extended this scheme iteratively by other groups, such as public institutions (municipality, federal state, country). An overview of the final classes can be found in "00_data\classification\class_catalogue.xlsx". The classification was done with the help of code words. The order of classification was important because some code words could occur for different classes.

### 4. Specifying the classification and creating unique identifiers
The preliminary classification was then transformed into an improved hierarchical classification, which can also be found in "00_data\classification\class_catalogue.xlsx". In addition, we created a unique identifier for each owner by removing typical but superfluous words and characters (e.g. spaces, dots, "mit sitz in" etc.) from the name. For individuals, we used the family name and address to create a unique identifier. We used fuzzy matching to identify the same owner who appeared with slightly different spellings (e.g. due to misspellings or different abbreviations) to standardise their identifier. In some cases, the unique identifier was listed with the addresses due to different spellings, etc. We have chosen to assign the most frequent address to all occurrences of the identifier. 

### 5. Georeferencing the addresses
In order to be able to calculate the distances of the owners to their parcels later on, we georeferenced the addresses of all owners. The georeferencing was done in different ways (Google API, Nominatim, matching with OSM data, fuzzy matching of addresses) and the scripts for this are quite messy and will likely not work as they are provided in this repository. The reason for choosing different methods was that we didn't have enough money to georeference all addresses with the Google API, although it is probably the most accurate. Also, we didn't want to put too much load on the OSM server as they don't want that. Besides, it also took a long time with their API. So we downloaded all German OSM data to create a shapefile with all cities and villages in Germany and their postcodes. With the help of this shapefile, we also georeferenced a number of addresses at least to the city centre. All remaining addresses that were not previously georeferenced were also matched using a fuzzy matching approach of address names. We therefore recommend carrying out an analysis of distances only on the basis of distance classes.

### 6. Preparing the search of companies and other organizations in DAFNE
In order to identify all possible relationships between private individuals, companies and other organisations, we used the DAFNE company database. The database contains all information about the corporate structures of companies registered in Germany. We used the names of private companies and non-profit organisations, associations, foundations and others for a search in DAFNE. DAFNE allows uploading lists of 1000 company names, so we prepared these lists with all names in lower case and uploaded them manually. Before DAFNE presents all the information, it creates a table with the names entered, the corresponding matching company names and a DAFNE-specific ID. We saved this table so that in later steps we could match the entries coming from ALKIS with the matching names coming from DAFNE, as they did not have the same spelling (and to clean the company names, see 7.). As not all companies could be matched via this batch search, we looked for these names manually with different spellings. We searched for all possible information on boards of directors, supervisory boards, managing directors, subsidiaries and shareholders of companies. The output tables were manually combined into a single table.

### 7. Final clean-up of owners' names and addition of supplementary information
Our aim was to identify the agricultural landowners and to add the economic branches of the enterprises with their entries in ALKIS. This required a final clean-up of names and addresses, as some owners still appeared with different spellings or addresses. We created separate tables for the three different groups of public, church and non-profit landowners (see 3. and 4., this step is not included in the scripts). These tables contained all the unique entries within these three groups. They were used to manually unify the names of the owners where necessary. Then we identified all aristrocratic persons who often appeared in different spellings due to their aristrocratic names and positions. Their names and addresses were also manually cleaned up and unified in a separate file. The resulting files were then used to update the ALKIS data frame. The matching names from DAFNE were used to clean up the company names. After all the name cleaning, we created a new unique identifier for all owners and unified the addresses for all unique identifiers. 
After this, we added the supplementary information to the owner data frame. The supplementary information included information on the recipients of agricultural subsidies (sourced from [farmsubsidy.org](https://data.farmsubsidy.org/)), on all feed producers (sourced from [Bundesamt for Verbraucherschutz und Lebensmittelsicherheit](https://www.bvl.bund.de/DE/Arbeitsbereiche/02_Futtermittel/03_AntragstellerUnternehmen/01_Zulassungs_Registrierungspflicht/02_Futtermittelbetriebe_Verzeichnis/fm_FMBetriebeVerzeichnis_node.html)) and a classification of companies into economic branches, which was also available in DAFNE (a German overview can be found in "00_data_\classification_of_economic_branches_2008.png"). Using these three data sources, we were able to identify agricultural actors among the landowners as agricultural actors. 